---
title: "lfq_pipeline"
author: "Dain Brademan"
date: "2024-01-30"
output: html_document
---

# README

It is always good practice to start off any programming project with a quick *README* document or section. README documentation is a great way provide information to developers, users, or contributors of code that you wrote. Project overviews, purpose, setup instructions, and usage cases are great things to included in your README documents.

### Pipeline Overview

This R Markdown document was assembled to provide a one-size-fits-most solution for Huttenhain lab members to work up their mass spectrometry-based abundance proteomics data, specifically the data from APEX-enrichment experiments. The data that comes directly out from proteomics search engines like [Spectronaut](https://biognosys.com/software/spectronaut/) or [MaxQuant](https://www.maxquant.org/) are reported at the peptide or fragment ion level. These data need to be reassembled back to the protein level and require cleaning / curating before biological interpretation is conducted. We do this with a mix of custom code and a robust statistical framework named [MSstats](https://msstats.org/).

### Setup

Most of this pipeline can be used without major modification after the necessary packages have been installed. The installation of necessary packages is included in commented-out lines of code several sections below.

### Usage

This pipeline supports proteomics data searched using Spectronaut or MaxQuant. Use this pipeline only for abundance-based proteomics experiments. If you have PTM data, you will be better off using the PTM pipeline. You will need to following files in order to use this pipeline:

**Spectronaut Search**

-   Spectronaut Report (HuttenhainDIA_MsStats_FragmentIon)

-   This file is in .tsv format by default

**MaxQuant Search**

-   Maxquant Evidence File (.txt)

-   Maxquant Protein Groups File  (.txt)

-   Maxquant Annotations File (manually built)

# Installing / Loading Required R Libraries

When I'm working on a script from the ground up, the first thing I like to do is install (if not already installed) & load all the libraries that I'm planning on using before I even start thinking about loading and manipulating any data. R libraries only need to be installed once via the *install.packages("Function")*

### Installing CRAN Packages

Libraries only need to be installed once. After that, they're saved to your computer and you can view them under the **Packages** tab in the bottom right panel on your screen. How do you tell which libraries/packages you need to install? I usually just run the load statements below, and R will tell me if I don't have a package installed

The packages used in this code come from two different repositories, $\underline{C}omprehensive\space\underline{R}\space\underline{A}rchive\space\underline{N}etwork$ (CRAN) and BioConductor. They're installed differently.

CRAN packages can be installed several different ways. The first way is demonstrated in the code chunk below. Alternatively, you can click to the **Packages** tab over on the bottom right of the screen. You'll find an **Install** and **Update** button there that you can use to browse for packages

```{r}
# Don't need run this every time. 
# Only if you need to install a package.
# Just replace the Package in the parentheses with whatever you need installed
# If you get complaints about Rtools is required to build R packages, you can install it from the following URL: https://cran.r-project.org/bin/windows/Rtools/rtools42/rtools.html
# You don't really need to install it, but warnings are annoying

# install.packages("devtools")
# install.packages("data.table")
# install.packages("dplyr")
# install.packages("magrittr")
# install.packages("R.utils")
# install.packages("ggplot2")
# install.packages("ggrepel")
```

### Installing Bioconductor Packages

BioConductor has its own package manager. Before you can install anything from it, you first need to install the BioConductor manager from CRAN.

```{r}
# install.packages("BiocManager")
```

Then you can install all the BioConductor packages you want! Note, you may be prompted in the console window (bottom left) if you want to update packages. It's generally a good idea. Sometimes a package refuses to update. If that happens, close out of all your R tabs, clear your workspace, and try again. Sometimes incompatible packages can clash and cause really weird errors that are tough to troubleshoot, so I'd recommend keeping things as updated as possible.

```{r}
# Install a specific version of BioConductor core packages
# Don't use this unless you know what you're doing...<BiocManager::install(version = "3.18)>

# Install latest version of BioConductor core packages
# BiocManager::install()

```

Some BioConductor packages don't install with the core distribution. Do install these add-ons, you can specifically request these packages be installed

```{r}
# Install a set of specific non-core package. Format: c("Package1", "Package2", etc....)
# BiocManager::install(c("MSstats", "ComplexHeatmap"))
```

### Load Packages

Now that in theory everything is installed. Let's load our packages. If you get any errors regarding **Package not found**, just go back and install it! A general rule is the fully lowercase libraries are from CRAN and the CamelCase libraries are from BioConductor

```{r}
# Load Packages
library(devtools)
library (data.table)
library(dplyr)
library (magrittr)
library(R.utils) # Actually a cran package

# Plotting packages
library (ggplot2)
library(ggrepel)
rotate.x.axis.text <- theme(axis.text.x = element_text(angle = 90,vjust = 0.5, hjust=1))

# MSstats Packages
library(MSstats)

# EnrichR Complex Heatmap Package
library (ComplexHeatmap)

# Krogan Lab Utility Functions- Load From GitHub
utils <- "https://raw.githubusercontent.com/HuttenhainLab/bp_utils/master/"
source(paste0(utils, "enrichmentTestFunctions.R"))
source(paste0(utils,"MSstats_V4_Functions.R"))

# Dain Utility Functions - Load From GitHub
#drb_utils <- "C:/Users/dainb/OneDrive/Documents/GitHub/drb_utils/"
drb_utils <- "https://raw.githubusercontent.com/HuttenhainLab/drb_utils/main/"
source(paste0(drb_utils,"data_management.R"))
source(paste0(drb_utils,"spectronaut_data_cleaning.R"))
source(paste0(drb_utils,"gene_ontology_enrichment.R"))

```

# Pipeline Parameters

Enter in values specific to your dataset in this block and run all blocks of code sequentially.

```{r}
# Data Type
# SP - Spectronaut
# MQ - MaxQuant
export.type <- "SP" 

# Appended to all exported data/figures
data.name <- "Example_LFQ"

# create subdirectories to store intermediate tables and figures
Create.Pipeline.Directories(data.name)

# Pipeline considers samples with the same "Replicate/BioReplicate" column to be biological replicates i.e. from the same patient
  # This is not true for a typical APEX experiment where we instead have technical replicates
  # If `is.case.control` = TRUE, pipeline will make replicate column unique by concatenating Condition & Replicate columns
  # If FALSE, pipeline will leave replicate columns as-is. 
  # This will impact the statistical model either way.
is.case.control = TRUE

# File paths to your mass spec data in either MaxQuant or Spectronaut format
if (export.type == "SP") {
  # Spectronaut specific parameters
  spectronaut.lfq.report <- "../example_data/example_data_spectronaut.tsv"
} else if (export.type == "MQ") {
  maxquant.evidence <- "../example_data/example_data_MQ_evidence.txt"
  maxquant.proteingroups <- "../example_data/example_data_MQ_proteinGroups.txt"
  maxquant.annotations <- "../example_data/example_data_MQ_annotations.csv"
} else {
  stop("Non-supported data type. Make sure data type is SP or MQ, otherwise pipeline will break.")
}

# Names of all possible contrasts will be generated from Condition names (formatted "ConditionName1-ConditionName2")
  ## Example: regexContrasts <- c("Fsk-DMSO", "H2O2_Fsk-Fsk", "H2O2-DMSO", "H2O2_Fsk-H2O2")
  ## This list should be strings that match your exact condition names that you entered in Spectronaut. 
  ## You can also use RegEex terms if you know what you're doing, but I find it more exact to use full condition names to avoid accidental     RegEx matches

global.regexContrasts <- c(
  # 0 minute APEX vs no biotinylation control
  "0m_40-ctrl_40", "0m_50-ctrl_50", "0m_60-ctrl_60", "0m_70-ctrl_70", "0m_80-ctrl_80",
  
  # 5 minute APEX vs no biotinylation control
  "5m_40-ctrl_40", "5m_50-ctrl_50", "5m_60-ctrl_60", "5m_70-ctrl_70", "5m_80-ctrl_80",
  
  # 5 minute APEX vs 0 minute APEX                          
  "5m_40-0m_40", "5m_50-0m_50", "5m_60-0m_60", "5m_70-0m_70", "5m_80-0m_80"
)


# Sometimes you want to remove certain runs because they're bad, or before data normalization. 
# You can also do this at the condition level for ease, but you'll need to modify the code in the post-normalization section to do this
# Specify the runs here. 
runs.to.remove <- c()
# Uncomment the below line to remove the "control" samples before median normalization
# runs.to.remove <- c("TT000243_AM04-01","TT000244_AM04-02","TT000245_AM04-03","TT000246_AM04-04","TT000247_AM04-05","TT000254_AM04-12","TT000255_AM04-13","TT000256_AM04-14","TT000257_AM04-15","TT000258_AM04-16")
```

# Import Data

This reads the proteomics search engine data verbatim and loads it into a data frame. You can see this data frame in the environment window. (default location: upper left)
```{r}

# Spectronaut specific data files
if (export.type == "SP") {
  spectronaut.lfq.report <- fread(spectronaut.lfq.report)
  
# MaxQuant specific data files
} else if (export.type == "MQ") {
  maxquant.evidence <- fread(maxquant.evidence)
  maxquant.proteingroups <- fread(maxquant.proteingroups)
  maxquant.annotations <- fread(maxquant.annotations)
}
```

### Clean data

It's recommended to include commonly observed contaminant proteins when processing your proteomic datasets. However, Spectronaut does not remove these entries for you. Fortunately, all contaminant protein accessions are prepended with the text **Cont\_** so we can go through each row in our dataset and toss that data ourselves.

```{r}
if (export.type == "SP") {
  # remove all contaminant proteins.
  substring_to_remove <- "Cont_"
  
  # Identify rows containing the contaminant substring
  rows_to_remove <- grep(substring_to_remove, spectronaut.lfq.report$PG.ProteinAccessions)
  
  # Remove rows using the above vector of row indices
  spectronaut.lfq.report <- spectronaut.lfq.report[-rows_to_remove, ]
  
  rm(rows_to_remove, substring_to_remove)
}
```


### Make protein metadata linker table

During data processing via msStats most of the metadata we'd like to include for visualization or functional enrichment analysis is discarded. I find it useful to create a linker table early on containing all the metadata I want to crossmap back in based on a unique identifier. The best one I've found is the Uniprot accession ID column AKA **PG.ProteinAccession**.

```{r}

# Spectronaut specific protein linker table
if (export.type == "SP") {
  
  # scrape a linker table for Uniprot IDs and Protein Names for later visualization
  proteinNameCrossLookup <- subset(spectronaut.lfq.report, select = c(PG.ProteinAccessions, PG.ProteinNames))
  
# MaxQuant specific protein linker table
} else if (export.type == "MQ") {
  
  # scrape a linker table for Uniprot IDs and Protein Names for later visualization
  proteinNameCrossLookup <- subset(maxquant.proteingroups, select = c(`Protein IDs`, `Gene names`))
}

# rename columns for easy merging later
colnames(proteinNameCrossLookup) <- c("Protein", "ProteinName")

# filter to distinct protein IDs. %>% is an operator from the package magrittr which can be used to chain multiple functions together.
proteinNameCrossLookup <- proteinNameCrossLookup %>% distinct(Protein, .keep_all = TRUE)

```

### Make Replicate column unique if set

This prevents MSstats from treating experimental/technical replicates as bioreplicates. Bioreplicates are modeled differently within the statistical framework.

```{r}
if (is.case.control) {
  spectronaut.lfq.report$R.Replicate <- paste(spectronaut.lfq.report$R.Condition, spectronaut.lfq.report$R.Replicate, sep = ".")
}
```

### Reformat data for msStats Data Processing

Most proteomic pipelines by default do not export data in the format that msStat expects for protein grouping and normalization. Fortunately for us, msStats already has a built-in converter function that we can take advantage of with minimal effort. While it's not the worst thing to reformat this data ourselves, work smarter, not harder.

After this stage, we will no longer need to write special code to handle Spectronaut vs MaxQuant data

```{r}
if (export.type == "SP") {
  
  # the optional arguments are recommended in the msStats user guide. Feel free to change if you know better.
  global.prepared <- SpectronauttoMSstatsFormat(spectronaut.lfq.report,
                                       filter_with_Qvalue = TRUE, ## same as default
                                       qvalue_cutoff = 0.01, ## same as default
                                       removeProtein_with1Feature = FALSE,
                                       use_log_file = FALSE
                                       )
  
  rm(spectronaut.lfq.report)
  
  Save.Csv.With.Timestamp(global.prepared, "CleanedPreprocessedData.csv", paste(data.name, "data", sep = "_"))
                          
} else if (export.type == "MQ") {
  
  global.prepared <- MaxQtoMSstatsFormat(maxquant.evidence, 
                                       maxquant.annotations, 
                                       maxquant.proteingroups)
  
  rm(maxquant.evidence, maxquant.annotations, maxquant.proteingroups)
}

```

### Remove any unwanted runs

This tosses any files listed in the *runs.to.remove* vector. I use this block of code to toss all my control samples before median normalization. 
```{r}
# Tosses any rows where the 'Run' column matches a value in the 'runs.to.remove' vector listed at the top
global.prepared <- global.prepared[!(global.prepared$Run %in% runs.to.remove), ]

```

# Quality Control Plots of the Raw Data

Now is a good time to take a look at the raw data.

### View a distribution of the peptide ion transitions.

This should be monomodal after the *SpectronauttoMSstatsFormat* filtering step

```{r}
hist(log2(global.prepared$Intensity), breaks = 100)
```

The code block below is just for demonstration purposes. Marking specific thresholds can be useful for quality control or visualization purposes.

```{r}
mean_intensity <- mean(global.prepared$Intensity, na.rm = TRUE)

# Plot histogram with vertical line marking the mean
ggplot(global.prepared, aes(x = log2(Intensity))) +
  geom_histogram(binwidth=0.1, color="black", fill="white") +
  geom_vline(xintercept = log2(mean_intensity), color="blue", linetype="dashed", size=1)
  

```

### Count unique peptide sequences per experimental condition

For a general abundance proteomics APEX experiment, we would hope for 70,000+ peptide sequences per replicate. This particular dataset is lower than that due to various reasons, but what we are looking for are consistent numbers across related replicates and the APEX samples. APEX control samples (no construct or no biotin so no enrichment) hopefully will be lower.

```{r}
unique_counts <- as.data.table(global.prepared) %>%
  filter(!is.na(Intensity)) %>%
  filter(Intensity != 0) %>%
  group_by(BioReplicate) %>%
  summarize(Unique_Peptide_Count = n_distinct(PeptideSequence))

ggplot(unique_counts, aes(x = BioReplicate, y = Unique_Peptide_Count)) +
  geom_bar(stat = "identity", fill = "skyblue", width = 0.5) +
  geom_text(aes(label = Unique_Peptide_Count), vjust = -0.5, color = "black", size = 3) +
  labs(title = "Unique Peptide Sequences per Replicate",
       x = "Replicate",
       y = "Number of Peptide Sequences per Replicate") +
  theme_bw() +
  rotate.x.axis.text

ggsave(paste(paste(data.name, "figures", sep = "_"), "UniqueFeaturesPerReplicate.pdf", sep = "/"))
```

### Get boxplot of quantitative values per sample

Just the same as the number of peptide sequences, we also expect related samples to have similar quantitative distributions. The APEX experimental workflow involved biotin labeling of proximal proteins and subsequent enrichment. As the control samples were not designed to undergo biotin labeling, the control samples ideally will have *lower* intensity distributions than the APEX samples.

```{r}
ggplot (data = global.prepared, aes (x = paste0(BioReplicate),
                          y = log10(Intensity), 
                          fill = Condition)) + 
  geom_boxplot() +
  rotate.x.axis.text # this is defined in the default template of all my Rmd files

ggsave(paste(paste(data.name, "figures", sep = "_"), "FeatureIntensitiesPerSample.pdf", sep = "/"))
```

# Use MSstats for protein summarization

This is where we summarize peptides (or peptide transitions depending on your data export format) back into protein groups. Depending on your experiment, you're going to want to turn normalization on or off. For your typical APEX experiment, the control condition doesn't have Biotin Phenol meaning biotin enrichment does not occur. We expect an entirely different population of proteins to be enriched for in the BP samples, so its best to not do any sort of normalization at this point.

```{r}
global.proteinSummarization = MSstats::dataProcess(global.prepared,
                               normalization = 'FALSE',	# "equalizeMedians"			
                               logTrans = 2,				
                               featureSubset = 'highQuality',				
                               summaryMethod="TMP",
                               censoredInt='NA',				
                               MBimpute=FALSE,				
                               maxQuantileforCensored=0.999)

Save.Csv.With.Timestamp(global.proteinSummarization$FeatureLevelData, "FeatureLevelData.csv", paste(data.name, "data", sep = "_"))
Save.Csv.With.Timestamp(global.proteinSummarization$ProteinLevelData, "ProteinLevelData.csv", paste(data.name, "data", sep = "_"))
```

# (Optional) Reload DataProcess results

If you ever want to go back & reanalyze data after the data process function, it can be frustrating to have to run through the entire pipeline again up to this point.

```{r}
# global.proteinSummarization <- list(ProteinLevelData = NULL, FeatureLevelData = NULL)

# global.proteinSummarization$ProteinLevelData <- fread(paste0(paste(data.name, "data/", sep = "_"), "20240326_ProteinLevelData.csv"))
# global.proteinSummarization$FeatureLevelData <- fread(paste0(paste(data.name, "data/", sep = "_"), "20240326_FeatureLevelData.csv"))

# global.proteinSummarization$ProteinLevelData$RUN <- factor(global.proteinSummarization$ProteinLevelData$RUN)
# global.proteinSummarization$ProteinLevelData$Protein <- factor(global.proteinSummarization$ProteinLevelData$Protein)
# global.proteinSummarization$ProteinLevelData$GROUP <- factor(global.proteinSummarization$ProteinLevelData$GROUP)

```

# Plot protein intensities pre-normalization

All of our peptide-level data has now been summarized into protein groups. Let's inspect the intensity distributions of this data. Note: the data process function already log2 transformed this data, so we don't have to ourselves.

```{r}

ggplot (global.proteinSummarization$ProteinLevelData, 
        aes (x = interaction (SUBJECT, GROUP), 
             y = LogIntensities, 
             fill = GROUP)
        ) + 
  geom_boxplot() + 
  rotate.x.axis.text

ggsave(paste(paste(data.name, "figures", sep = "_"), "ProteinIntensitiesPerSample.pdf", sep = "/"))

```

```{r}
unique_counts <- global.proteinSummarization$ProteinLevelData %>%
  filter(!is.na(LogIntensities)) %>%
  filter(!is.infinite(LogIntensities)) %>%
  group_by(GROUP) %>%
  summarize(Unique_Protein_Count = n_distinct(Protein))

ggplot(unique_counts, aes(x = GROUP, y = Unique_Protein_Count)) +
  geom_bar(stat = "identity", fill = "skyblue", width = 0.5) +
  labs(title = "Count of Protein Groups per Condition",
       x = "Condition",
       y = "Number of Protein Groups") +
  geom_vline(xintercept = 5.5, linetype = "dashed", color = "darkgray") +
  geom_vline(xintercept = 10.5, linetype = "dashed", color = "darkgray") +
  rotate.x.axis.text

ggsave(paste(paste(data.name, "figures", sep = "_"), "UniqueProteinGroupsPerCondition.pdf", sep = "/"))
```

# Plot average protein CVs per condition

```{r}
proteomics_data <- global.proteinSummarization$ProteinLevelData %>%
  group_by(GROUP, Protein) %>%
  summarize(CV = sd(LogIntensities) / mean(LogIntensities) * 100)

# Plot the distribution of CV for each group
ggplot(proteomics_data, aes(x = GROUP, y = CV)) +
  geom_boxplot() +
  labs(title = "Coefficient of Variation Distribution by Group",
       x = "Please note CV's were calculated in Log2 space, not raw intensities",
       y = "Coefficient of Variation (%)") +
  rotate.x.axis.text

ggsave(paste(paste(data.name, "figures", sep = "_"), "ProteinCVs.pdf", sep = "/"))
```

# Native Carboxylase Enrichment

This is a quality control plot I borrowed from Ben Polacco.If you've normalized your data, it's useful. Without normalization the results are pretty difficult to interpret.

*APEX works by labeling neighboring/interacting proteins with biotin which is then used to purify labeled proteins. There are also proteins that are endogenously biotinylated which will co-purify with the APEX-labeled proteins. Here we look at a subset of these endogenous biotin proteins, and we inspect their post-normalization background levels, which are inversely related to the labeling-efficiency of APEX. More background after normalization implies there is less APEX-labeled signal.*

If you see your control conditions at higher abundance compared to the rest of your samples, great, APEX labeling and enrichment is behaving as we expected!

```{r}

biotin.carboxylases.up <- c("O00763","P05165","P11498","Q13085","Q96RQ3")

native.biotin.data <- as.data.table(global.proteinSummarization$ProteinLevelData)
native.biotin.data <- native.biotin.data[grepl(paste(biotin.carboxylases.up, collapse = "|"), native.biotin.data$Protein), ]


# Plotting code
ggplot(native.biotin.data, aes(x = Protein, y = LogIntensities, fill = GROUP)) +
  stat_summary(
    fun.data = mean_sdl,
    fun.args = list(mult = 1),
    geom = "bar",
    position = position_dodge(width = 0.8),  # Adjust the width as needed
    width = 0.6  # Adjust the width of bars
  ) +
  stat_summary(
    fun.data = mean_sdl,
    fun.args = list(mult = 1),
    geom = "errorbar",
    position = position_dodge(width = 0.8),
    width = 0.4  # Adjust the width of error bars
  ) +
  labs(title = "Mean Log Intensities with Standard Deviation by GROUP",
       x = "GROUP", y = "Log Intensities") +
  theme_bw()

ggsave(paste(paste(data.name, "figures", sep = "_"), "NativeCarboxylases.pdf", sep = "/"))

rm(biotin.carboxylases.up, test)
```

# Do global pairwise comparisons to check for successful APEX enrichment

Another way you can check for successful APEX enrichment is to make volcano plots comparing protein abundances between non-BP vs BP conditions. What you should see is a mass up-regulation of most proteins. If you don't see that, your enrichment likely didn't work, or you have a ton of background binding. To make these comparisons, we can use the GroupComparison function from msStats. To use this function, you need the output from MSstats::DataProcess, and you also have to build a contrast matrix, essentially a numerical vector indicating what experimental conditions you want to compare.

I adapted Ben Polacco's old *makeContrast* function to help build the group comparison matrix without errors. Again, you can do this yourself if you're careful without using this function.

```{r}

# makes an MsStats-compatible contrast matrix using the condition comparisons in the parameter block
contrast.matrix <- Make.LFQ.Contrast.Matrix (input.data.frame = global.proteinSummarization, condition.vector = global.regexContrasts)
# Model-based comparison
# Do pairwise comparison via MSstats using the provide contrast matrix, then select the comparison results

global.pairwiseComparison <- MSstats::groupComparison(contrast.matrix, global.proteinSummarization)$ComparisonResult
global.pairwiseComparison <- merge(global.pairwiseComparison, proteinNameCrossLookup, by = "Protein", all.x = TRUE)

Save.Csv.With.Timestamp(global.pairwiseComparison, "GroupComparisonsData.csv", paste(data.name, "data", sep = "_"))

```

# Volcano Plots

Do all the volcano plots for all experiment contrasts

```{r}

# This is essentially a foreach loop. 
# Takes each of the labels of the above pairwise comparison
# Subselects that specific data
# Finally, makes the volcano plot
lapply(global.regexContrasts, function(comparison) {
  
  # define significant proteins
  thisPairwiseComparison <- as.data.table(global.pairwiseComparison %>% filter(Label == comparison))
  
  # Merge human readable protein names in case they're not there.
  
  ## This chunks adds a new column to our pairwise comparison data table
  # For all proteins with |Log2FC| < 1 and pval > 0.05, value is "Not"
  # For proteins with |Log2FC| >= 1 and pval <= 0.05, value is "Up" or "Down" as appropriate
  thisPairwiseComparison[, Significance := "Not"]
  thisPairwiseComparison[pvalue < 0.05 & abs(log2FC) > log2(1.5),
         Significance := ifelse (log2FC > 0, "Up", "Down")]
  
  ## Render volcano plots. Highlight arrestin and adrenoceptor beta 2
  plot <- ggplot(thisPairwiseComparison, aes(x = log2FC, y = -log10(pvalue), color = Significance, label = ProteinName)) +
    
    # significance labels and lines
    # vertical lines
    geom_vline(xintercept = c(-log2(1.5), log2(1.5)), linetype = "dashed", color = "darkgray") +
    annotate("text", x = c(-log2(1.5), log2(1.5)), y = 0, label = c("-Log2(1.5)", "Log2(1.5)"), vjust = 1, hjust = 0.5, color = "darkgray") +
    
    # horizontal line
    geom_hline(yintercept = -log10(0.05), linetype = "dashed", color = "darkgray") +
    annotate("text", x = Inf, y = -log10(0.05), label = "p-val=0.05", hjust = 1, color = "darkgray") +
  
    # circles representing proteins
    geom_point() +
    scale_color_manual(values = c(Not = "gray", Down = "blue", Up = "red")) +
    ggrepel::geom_text_repel(data = thisPairwiseComparison[Significance != "Not"], size = 2, max.overlaps = 20) +
    
    # highlight people's favorite protein
    geom_point(data = thisPairwiseComparison[thisPairwiseComparison$ProteinName == "ARRB2;DKFZp686L0365", ],
               aes(x = log2FC, y = -log10(pvalue)), color = "green", size = 3) +
    geom_text_repel(data = thisPairwiseComparison[thisPairwiseComparison$ProteinName == "ARRB2;DKFZp686L0365", ],
                    aes(x = log2FC, y = -log10(pvalue)), label = "ARRB2_HUMAN", color = "green", size = 3, box.padding = 0.5) +
    
    geom_point(data = thisPairwiseComparison[thisPairwiseComparison$ProteinName == "EEA1", ],
               aes(x = log2FC, y = -log10(pvalue)), color = "green", size = 3) +
    geom_text_repel(data = thisPairwiseComparison[thisPairwiseComparison$ProteinName == "EEA1", ],
                    aes(x = log2FC, y = -log10(pvalue)), label = "EEA1_HUMAN", color = "green", size = 3, box.padding = 0.5) +
    theme_bw()
  
  print(plot)
  
  ggsave(paste("./VolcanoPlot", paste(comparison, ".pdf", sep = ""), sep = "_"), plot)
})

```

### Heatmaps

```{r}

# filter for significant changers
# make heat map
# Go enrichment on clusters
pval.cutoff <- 0.05
FC.cutoff <- 1

# filter all proteins in group comparisons to having at least 1 significant changer in for each of our groups
Filtered.Proteins <- as.data.table(
  global.pairwiseComparison %>% 
    group_by(ProteinName) %>% 
    filter(any(adj.pvalue <= pval.cutoff & abs(log2FC) >= FC.cutoff))
)

Sig.Changing.Protein.Matrix <- as.matrix(dcast(Filtered.Proteins, Protein ~ Label, value.var = "log2FC"), rownames = "Protein")
Sig.Changing.Protein.Matrix[is.infinite(Sig.Changing.Protein.Matrix)] <- 0
Sig.Changing.Protein.Matrix[is.na(Sig.Changing.Protein.Matrix)] <- 0

# UNCOMMENT TO SAVE HEATMAP
#pdf("HeatMap_ADJUSTED.pdf")

# Now create the heatmap

heatmap <- Heatmap(
  Sig.Changing.Protein.Matrix,
  row_title = sprintf("%d proteins with\nadj.pval < 0.05 & |Log2FC| > 1", nrow(Sig.Changing.Protein.Matrix)),
  name = "Log2FC of Labeled Timepoints\n",
  show_row_names = FALSE,
  cluster_columns = TRUE,
  #km = 4
)

drawnHeatmap <- draw(heatmap)

#dev.off()
```
